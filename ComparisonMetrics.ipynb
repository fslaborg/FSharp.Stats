
        {
            "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": []
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": null, "outputs": [],
           "source": ["#r \"nuget: Plotly.NET, 2.0.0-preview.16\"\n",
"#r \"nuget: Plotly.NET.Interactive, 2.0.0-preview.16\"\n",
"#r \"nuget: FSharp.Stats\"\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["# Evaluating predictions and tests\n",
"\n",
"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fslaborg/FSharp.Stats/gh-pages?urlpath=/tree/home/jovyan/Integration.ipynb)\n",
"\n",
"#### Table of contents\n",
"\n",
"* [Confusion matrices](#Confusion-matrices)\n",
"\n",
"  * [Binary confusion matrix](#Binary-confusion-matrix)\n",
"  \n",
"  * [Multi-label confusion matrix](#Multi-label-confusion-matrix)\n",
"  \n",
"\n",
"* [Comparison-Metric](#Comparison-Metrics)\n",
"\n",
"  * [ComparisonMetrics for binary comparisons](#ComparisonMetrics-for-binary-comparisons)\n",
"  \n",
"  * [ComparisonMetrics for multi-label comparisons](#ComparisonMetrics-for-multi-label-comparisons)\n",
"  \n",
"    * [Macro-averaging metrics](#Macro-averaging-metrics)\n",
"    \n",
"    * [Micro-averaging metrics](#Micro-averaging-metrics)\n",
"    \n",
"  \n",
"  * [Creating threshold-dependent metric maps](#Creating-threshold-dependent-metric-maps)\n",
"  \n",
"    * [For binary predictions](#For-binary-predictions)\n",
"    \n",
"    * [For multi-label predictions](#For-multi-label-predictions)\n",
"    \n",
"    * [ROC curve example](#ROC-curve-example)\n",
"    \n",
"  \n",
"\n",
"FSharp.Stats contains a collection for assessing both binary and multi-label comparisons, for example the results of a binary/multi-label classification or the results of a statistical test.\n",
"\n",
"Usually, using the functions provided by the `ComparisonMetrics` module should be enough, but for clarity this documentation also introduces the `BinaryConfusionMatrix` and `MultiLabelConfusionMatrix` types that are used to derive the `ComparisonMetrics.`\n",
"\n",
"## Confusion matrices\n",
"\n",
"See also: [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
"\n",
"Confusion matrices can be used to count and visualize the outcomes of a prediction against the actual \u0027true\u0027 values and therefore assess the prediction quality.\n",
"\n",
"Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa. The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another).\n",
"\n",
"### Binary confusion matrix\n",
"\n",
"A binary confusion matrix is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table).\n",
"\n",
"let for example the actual labels be the set\n",
"\n",
"\\begin{equation}\n",
"actual = (1,1,1,1,0,0,0)\n",
"\\end{equation}\n",
"\n",
"and the predictions\n",
"\n",
"\\begin{equation}\n",
"predicted = (1,1,1,0,1,0,0)\n",
"\\end{equation}\n",
"\n",
"a binary confusion matrix can be filled by comparing actual and predicted values at their respective indices:\n",
"\n",
"predicted\n",
"--- | --- | --- | ---\n",
"\u0026#32; | \u0026#32; | True | False\n",
"actual | True | 3 | 1\n",
"\u0026#32; | False | 1 | 2\n",
"\n",
"\n",
"A whole array of prediction/test evaluation metrics can be derived from binary confusion matrices, which are all based on the 4 values of the confusion matrix:\n",
"\n",
"* TP (True Positives, the actual true labels predicted correctly as true)\n",
"\n",
"* TN (True Negatives, the actual false labels predicted correctly as false)\n",
"\n",
"* FP (False Positives, the actual false labels incorrectly predicted as true)\n",
"\n",
"* TP (False Negatives, the actual true labels incorrectly predicted as false)\n",
"\n",
"Predicted\n",
"--- | --- | --- | ---\n",
"\u0026#32; | \u0026#32; | True | False\n",
"Actual | True | TP | FN\n",
"\u0026#32; | False | FP | TN\n",
"\n",
"\n",
"These 4 base metrics are in principle what comprises the record type `BinaryConfusionMatrix`.\n",
"\n",
"A BinaryConfusionMatrix can be created in various ways :\n",
"\n",
"* from predictions and actual labels of any type using `BinaryConfusionMatrix.fromPredictions`, additionally passing which label is the \"positive\" label\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 3, "outputs": [
          {
           "data": {
            "text/plain": ["{ TP = 3",
"  TN = 2",
"  FP = 1",
"  FN = 1 }"]
        },
           "execution_count": 3,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["let actual = [1;1;1;1;0;0;0]\n",
"let predicted = [1;1;1;0;1;0;0]\n",
"\n",
"open FSharp.Stats.Testing\n",
"\n",
"BinaryConfusionMatrix.ofPredictions(1,actual,predicted)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["* from boolean predictions and actual labels using `BinaryConfusionMatrix.fromPredictions`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 4, "outputs": [
          {
           "data": {
            "text/plain": ["{ TP = 3",
"  TN = 2",
"  FP = 1",
"  FN = 1 }"]
        },
           "execution_count": 4,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["let actualBool = [true;true;true;true;false;false;false]\n",
"let predictedBool = [true;true;true;false;true;false;false]\n",
"\n",
"BinaryConfusionMatrix.ofPredictions(actualBool,predictedBool)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["* directly from obtained TP/TN/FP/FN values using `BinaryConfusionMatrix.create`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 5, "outputs": [
          {
           "data": {
            "text/plain": ["{ TP = 3",
"  TN = 2",
"  FP = 1",
"  FN = 1 }"]
        },
           "execution_count": 5,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["BinaryConfusionMatrix.create(tp=3,tn=2,fp=1,fn=1)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["There are more things you can do with `BinaryConfusionMatrix`, but most use cases are covered and abstracted by `ComparisonMetrics` (see [below](#Comparison-metrics)).\n",
"\n",
"### Multi label confusion matrix\n",
"\n",
"Confusion matrix is not limited to binary classification and can be used in multi-class classifiers as well, increasing both dimensions by the amount of additional labels.\n",
"\n",
"let for example the actual labels be the set\n",
"\n",
"\\begin{equation}\n",
"actual = (A,A,A,A,A,B,B,B,C,C,C,C,C,C)\n",
"\\end{equation}\n",
"\n",
"and the predictions\n",
"\n",
"\\begin{equation}\n",
"predicted = (A,A,A,B,C,B,B,A,C,C,C,C,A,A)\n",
"\\end{equation}\n",
"\n",
"a multi-label confusion matrix can be filled by comparing actual and predicted values at their respective indices:\n",
"\n",
"Predicted\n",
"--- | --- | --- | --- | ---\n",
"\u0026#32; | \u0026#32; | Label A | Label B | Label C\n",
"Actual | Label A | 3 | 1 | 1\n",
"\u0026#32; | Label B | 1 | 2 | 0\n",
"\u0026#32; | Label C | 2 | 0 | 4\n",
"\n",
"\n",
"A `MultiLabelConfusionMatrix` can be created either\n",
"\n",
"* from the labels and a confusion matrix (`Matrix\u003cint\u003e`, note that the index in the label array will be assigned for the column/row indices of the matrix, and that the matrix must be square and of the same dimensions as the label array)\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 6, "outputs": [],
           "source": ["open FSharp.Stats\n",
"\n",
"let mlcm =\n",
"    MultiLabelConfusionMatrix.create(\n",
"        labels = [|\"A\"; \"B\"; \"C\"|],\n",
"        confusion =(\n",
"            [\n",
"                [3; 1; 1]\n",
"                [1; 2; 0]\n",
"                [2; 0; 4]\n",
"            ]\n",
"            |\u003e array2D\n",
"            |\u003e Matrix.Generic.ofArray2D\n",
"        )\n",
"    )\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["* from predictions and actual labels of any type using `MultiLabelConfusionMatrix.ofPredictions`, additionally passing the labels\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 7, "outputs": [
          {
           "data": {
            "text/plain": ["{ Labels = [|\"A\"; \"B\"; \"C\"|]",
"  Confusion =",
"   ",
"",
"      0 1 2",
"",
"           ",
"",
" 0 -\u003e 3 1 1",
"",
" 1 -\u003e 1 2 0",
"",
" 2 -\u003e 2 0 4",
"",
"",
"",
"Matrix of 3 rows x 3 columns}"]
        },
           "execution_count": 7,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["MultiLabelConfusionMatrix.ofPredictions(\n",
"    labels = [|\"A\"; \"B\"; \"C\"|],\n",
"    actual = [|\"A\"; \"A\"; \"A\"; \"A\"; \"A\"; \"B\"; \"B\"; \"B\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"|],\n",
"    predictions = [|\"A\"; \"A\"; \"A\"; \"B\"; \"C\"; \"B\"; \"B\"; \"A\"; \"C\"; \"C\"; \"C\"; \"C\"; \"A\"; \"A\"|]\n",
")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["It is however not as easy to extract comparable metrics directly from this matrix.\n",
"\n",
"Therefore, multi-label classification are most often compared using `one/all-vs-rest` and `micro/macro averaging` of metrics.\n",
"\n",
"It is possible to derive binary `one-vs-rest` confusion matrices to evaluate prediction metrics of individual labels from a multi-label confusion matrix.\n",
"\n",
"This is done by taking all occurences of the label in the actual labels as positive values, and all other label occurences as negatives. The same is done for the prediction vector.\n",
"\n",
"As an example, the derived binary confusion matrix for `Label A` in above example would be:\n",
"\n",
"Predicted\n",
"--- | --- | --- | ---\n",
"\u0026#32; | \u0026#32; | is A | is not A\n",
"Actual | is A | 3 | 2\n",
"\u0026#32; | is not A | 3 | 6\n",
"\n",
"\n",
"Programmatically, this can be done via `MultiLabelConfusionMatrix.oneVsRest`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 8, "outputs": [
          {
           "data": {
            "text/plain": ["{ TP = 3",
"  TN = 6",
"  FP = 3",
"  FN = 2 }"]
        },
           "execution_count": 8,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["mlcm\n",
"|\u003e MultiLabelConfusionMatrix.oneVsRest \"A\"\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Binary confusion matrices for all labels can be obtained by `MultiLabelConfusionMatrix.allVsAll`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 9, "outputs": [
          {
           "data": {
            "text/plain": ["A:",
"{ TP = 3",
"  TN = 6",
"  FP = 3",
"  FN = 2 }",
"B:",
"{ TP = 2",
"  TN = 10",
"  FP = 1",
"  FN = 1 }",
"C:",
"{ TP = 4",
"  TN = 7",
"  FP = 1",
"  FN = 2 }"]
        },
           "execution_count": 9,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["mlcm\n",
"|\u003e MultiLabelConfusionMatrix.allVsAll\n",
"|\u003e Array.iter (fun (label, cm) -\u003e printf $\"{label}:\\n{cm}\\n\")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["## Comparison Metrics\n",
"\n",
"`Comparison Metrics` is a record type that contains (besides other things) the 21 metric shown in the table below.\n",
"\n",
"It also provides static methods to perform calculation of individual metrics derived from a BinaryConfusionMatrix via the `ComparisonMetrics.calculate\u003cMetric\u003e` functions:\n",
"\n",
"Metric | Formula | API reference\n",
"--- | --- | ---\n",
"Sensitivity (TPR) | $TPR = \\frac{TP}{TP+TN}$ | [ComparisonMetrics.calculateSensitivity            ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateSensitivity)\n",
"Specificity (TNR) | $TNR = \\frac{TN}{TN+TP}$ | [ComparisonMetrics.calculateSpecificity            ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateSpecificity)\n",
"Precision (PPV) | $PPV = \\frac{TP}{TP+FP}$ | [ComparisonMetrics.calculatePrecision              ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculatePrecision)\n",
"NegativePredictiveValue (NPV) | $NPV = \\frac{TN}{TN+FN}$ | [ComparisonMetrics.calculateNegativePredictiveValue](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateNegativePredictiveValue)\n",
"Missrate (FNR) | $FNR = \\frac{FN}{FN+TP}$ | [ComparisonMetrics.calculateMissrate               ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateMissrate)\n",
"FallOut (FPR) | $FPR = \\frac{FP}{FP+TN}$ | [ComparisonMetrics.calculateFallOut                ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateFallOut)\n",
"FalseDiscoveryRate (FDR) | $FDR = \\frac{FP}{FP+TP}$ | [ComparisonMetrics.calculateFalseDiscoveryRate     ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateFalseDiscoveryRate)\n",
"FalseOmissionRate (FOR) | $FOR = \\frac{FN}{FN+TN}$ | [ComparisonMetrics.calculateFalseOmissionRate      ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateFalseOmissionRate)\n",
"PositiveLikelihoodRatio (LR+) | $LR+ = \\frac{TPR}{FPR}$ | [ComparisonMetrics.calculatePositiveLikelihoodRatio](/reference/fsharp-stats-testing-comparisonmetrics.html#calculatePositiveLikelihoodRatio)\n",
"NegativeLikelihoodRatio (LR-) | $LR- = \\frac{FNR}{TNR}$ | [ComparisonMetrics.calculateNegativeLikelihoodRatio](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateNegativeLikelihoodRatio)\n",
"PrevalenceThreshold (PT) | $PT = \\frac{\\sqrt{FPR}}{\\sqrt{TPR}+\\sqrt{FPR}}$ | [ComparisonMetrics.calculatePrevalenceThreshold    ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculatePrevalenceThreshold)\n",
"ThreatScore (TS) | $TS = \\frac{TP}{TP+FN+FP}$ | [ComparisonMetrics.calculateThreatScore            ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateThreatScore)\n",
"Prevalence | $Prevalence = \\frac{P}{P+N}$ | [ComparisonMetrics.calculatePrevalence             ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculatePrevalence)\n",
"Accuracy (ACC) | $ACC = \\frac{TP+TN}{TP+TN+FP+FN}$ | [ComparisonMetrics.calculateAccuracy               ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateAccuracy)\n",
"BalancedAccuracy (BA) | $BA = \\frac{TPR+TNR}{2}$ | [ComparisonMetrics.calculateBalancedAccuracy       ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateBalancedAccuracy)\n",
"F1 Score | $F1 = \\frac{2TP}{2TP+FP+FN}$ | [ComparisonMetrics.calculateF1                     ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateF1)\n",
"PhiCoefficient (MCC) | $MCC = \\frac{TP*TN-FP*FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$ | [ComparisonMetrics.calculatePhiCoefficient         ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculatePhiCoefficient)\n",
"FowlkesMallowsIndex (FM) | $FM = \\frac{}{}$ | [ComparisonMetrics.calculateFowlkesMallowsIndex    ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateFowlkesMallowsIndex)\n",
"Informedness (BM) | $BM = \\frac{}{}$ | [ComparisonMetrics.calculateInformedness           ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateInformedness)\n",
"Markedness (MK) | $MK = \\frac{}{}$ | [ComparisonMetrics.calculateMarkedness             ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateMarkedness)\n",
"DiagnosticOddsRatio (DOR) | $DOR = \\frac{}{}$ | [ComparisonMetrics.calculateDiagnosticOddsRatio    ](/reference/fsharp-stats-testing-comparisonmetrics.html#calculateDiagnosticOddsRatio)\n",
"\n",
"\n",
"### ComparisonMetrics for binary comparisons\n",
"\n",
"You can create the `ComparisonMetrics` record in various ways:\n",
"\n",
"* directly from obtained TP/TN/FP/FN values using `ComparisonMetrics.create`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 10, "outputs": [],
           "source": ["ComparisonMetrics.create(3,2,1,1)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["* From a `BinaryConfusionMatrix` using `ComparisonMetrics.create`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 11, "outputs": [],
           "source": ["let bcm = BinaryConfusionMatrix.ofPredictions(1,actual,predicted)\n",
"ComparisonMetrics.create(bcm)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["* from predictions and actual labels of any type using `ComparisonMetrics.ofBinaryPredictions`, additionally passing which label is the \"positive\" label\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 12, "outputs": [],
           "source": ["ComparisonMetrics.ofBinaryPredictions(1,actual,predicted)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["* from boolean predictions and actual labels using `BinaryConfusionMatrix.ofBinaryPredictions`\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 13, "outputs": [
          {
           "data": {
            "text/plain": ["{ P = 4.0",
"  N = 3.0",
"  SampleSize = 7.0",
"  TP = 3.0",
"  TN = 2.0",
"  FP = 1.0",
"  FN = 1.0",
"  Sensitivity = 0.75",
"  Specificity = 0.6666666667",
"  Precision = 0.75",
"  NegativePredictiveValue = 0.6666666667",
"  Missrate = 0.25",
"  FallOut = 0.3333333333",
"  FalseDiscoveryRate = 0.25",
"  FalseOmissionRate = 0.3333333333",
"  PositiveLikelihoodRatio = 2.25",
"  NegativeLikelihoodRatio = 0.375",
"  PrevalenceThreshold = 0.4",
"  ThreatScore = 0.6",
"  Prevalence = 0.5714285714",
"  Accuracy = 0.7142857143",
"  BalancedAccuracy = 0.7083333333",
"  F1 = 0.75",
"  PhiCoefficient = 0.4166666667",
"  FowlkesMallowsIndex = 0.75",
"  Informedness = 0.4166666667",
"  Markedness = 0.4166666667",
"  DiagnosticOddsRatio = 6.0 }"]
        },
           "execution_count": 13,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["ComparisonMetrics.ofBinaryPredictions(actualBool, predictedBool)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["### ComparisonMetrics for multi-label comparisons\n",
"\n",
"see also: [https://cran.r-project.org/web/packages/yardstick/vignettes/multiclass.html](https://cran.r-project.org/web/packages/yardstick/vignettes/multiclass.html)\n",
"\n",
"To evaluate individual label prediction metrics, you can create comparison metrics for each individual label confusion matrix obtained by `MultiLabelConfusionMatrix.allVsAll`:\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 14, "outputs": [
          {
           "data": {
            "text/plain": ["Label A:",
"\tSpecificity:0.667",
"\tAccuracy:0.643",
"Label B:",
"\tSpecificity:0.909",
"\tAccuracy:0.857",
"Label C:",
"\tSpecificity:0.875",
"\tAccuracy:0.786"]
        },
           "execution_count": 14,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["mlcm\n",
"|\u003e MultiLabelConfusionMatrix.allVsAll\n",
"|\u003e Array.map (fun (label,cm) -\u003e label, ComparisonMetrics.create(cm))\n",
"|\u003e Array.iter(fun (label,metrics) -\u003e printf $\"Label {label}:\\n\\tSpecificity:%.3f{metrics.Specificity}\\n\\tAccuracy:%.3f{metrics.Accuracy}\\n\")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### Macro-averaging metrics\n",
"\n",
"Macro averaging averages the metrics obtained by calculating the metric of interest for each `one-vs-rest` binary confusion matrix created from the multi-label confusion matrix..\n",
"\n",
"So if you for example want to calculate the macro-average Sensitivity(TPR) $TPR_{macro}$ of a multi-label prediction, this is obtained by averaging the $TPR_i$ of each individual `one-vs-rest` label prediction for all $i = 1 .. k$ labels:\n",
"\n",
"$$TPR_{macro} = \\frac1k\\sum_{i=1}^{k}TPR_i$$\n",
"\n",
"macro average metrics can be obtained either from multiple metrics, a multi-label confusion matrix, or a sequence of binary confusion matrices\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 15, "outputs": [],
           "source": ["ComparisonMetrics.macroAverage([ComparisonMetrics.create(3,6,3,2); ComparisonMetrics.create(2,10,1,1); ComparisonMetrics.create(4,7,1,2)] )\n",
"ComparisonMetrics.macroAverage(mlcm)\n",
"ComparisonMetrics.macroAverage(mlcm |\u003e MultiLabelConfusionMatrix.allVsAll |\u003e Array.map snd)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["or directly from predictions and actual labels of any type using `ComparisonMetrics.macroAverageOfMultiLabelPredictions`, additionally passing the labels\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 16, "outputs": [
          {
           "data": {
            "text/plain": ["{ P = 4.666666667",
"  N = 9.333333333",
"  SampleSize = 14.0",
"  TP = 3.0",
"  TN = 7.666666667",
"  FP = 1.666666667",
"  FN = 1.666666667",
"  Sensitivity = 0.6444444444",
"  Specificity = 0.8169191919",
"  Precision = 0.6555555556",
"  NegativePredictiveValue = 0.8122895623",
"  Missrate = 0.3555555556",
"  FallOut = 0.1830808081",
"  FalseDiscoveryRate = 0.3444444444",
"  FalseOmissionRate = 0.1877104377",
"  PositiveLikelihoodRatio = 4.822222222",
"  NegativeLikelihoodRatio = 0.4492063492",
"  PrevalenceThreshold = 0.3329688981",
"  ThreatScore = 0.4821428571",
"  Prevalence = 0.3333333333",
"  Accuracy = 0.7619047619",
"  BalancedAccuracy = 0.7306818182",
"  F1 = 0.6464646465",
"  PhiCoefficient = 0.4644624644",
"  FowlkesMallowsIndex = 0.6482286558",
"  Informedness = 0.4613636364",
"  Markedness = 0.4678451178",
"  DiagnosticOddsRatio = 12.33333333 }"]
        },
           "execution_count": 16,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["ComparisonMetrics.macroAverageOfMultiLabelPredictions(\n",
"    labels = [|\"A\"; \"B\"; \"C\"|],\n",
"    actual = [|\"A\"; \"A\"; \"A\"; \"A\"; \"A\"; \"B\"; \"B\"; \"B\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"|],\n",
"    predictions = [|\"A\"; \"A\"; \"A\"; \"B\"; \"C\"; \"B\"; \"B\"; \"A\"; \"C\"; \"C\"; \"C\"; \"C\"; \"A\"; \"A\"|]\n",
")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### Micro-averaging metrics\n",
"\n",
"Micro aggregates the `one-vs-rest` binary confusion matrices created from the multi-label confusion matrix, and then calculates the metric from the aggregated (TP/TN/FP/FN) values.\n",
"\n",
"So if you for example want to calculate the micro-average Sensitivity(TPR) $TPR_{micro}$ of a multi-label prediction, this is obtained by summing each individual `one-vs-rest` label prediction\u0027s $TP$ and $TN$ and obtaining $TPR_{micro}$ by\n",
"\n",
"$$TPR_{micro} = \\frac{TP_1 + TP_2 .. + TP_k}{(TP_1 + TP_2 .. + TP_k)+(TN_1 + TN_2 .. + TN_k)}$$\n",
"\n",
"micro average metrics can be obtained either from multiple binary confusion matrices or a multi-label confusion matrix\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 17, "outputs": [],
           "source": ["ComparisonMetrics.microAverage([BinaryConfusionMatrix.create(3,6,3,2); BinaryConfusionMatrix.create(2,10,1,1); BinaryConfusionMatrix.create(4,7,1,2)] )\n",
"ComparisonMetrics.microAverage(mlcm)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["or directly from predictions and actual labels of any type using `ComparisonMetrics.macroAverageOfMultiLabelPredictions`, additionally passing the labels\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 18, "outputs": [
          {
           "data": {
            "text/plain": ["{ P = 14.0",
"  N = 28.0",
"  SampleSize = 42.0",
"  TP = 9.0",
"  TN = 23.0",
"  FP = 5.0",
"  FN = 5.0",
"  Sensitivity = 0.6428571429",
"  Specificity = 0.8214285714",
"  Precision = 0.6428571429",
"  NegativePredictiveValue = 0.8214285714",
"  Missrate = 0.3571428571",
"  FallOut = 0.1785714286",
"  FalseDiscoveryRate = 0.3571428571",
"  FalseOmissionRate = 0.1785714286",
"  PositiveLikelihoodRatio = 3.6",
"  NegativeLikelihoodRatio = 0.4347826087",
"  PrevalenceThreshold = 0.3451409985",
"  ThreatScore = 0.4736842105",
"  Prevalence = 0.3333333333",
"  Accuracy = 0.7619047619",
"  BalancedAccuracy = 0.7321428571",
"  F1 = 0.6428571429",
"  PhiCoefficient = 0.4642857143",
"  FowlkesMallowsIndex = 0.6428571429",
"  Informedness = 0.4642857143",
"  Markedness = 0.4642857143",
"  DiagnosticOddsRatio = 8.28 }"]
        },
           "execution_count": 18,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["ComparisonMetrics.microAverageOfMultiLabelPredictions(\n",
"    labels = [|\"A\"; \"B\"; \"C\"|],\n",
"    actual = [|\"A\"; \"A\"; \"A\"; \"A\"; \"A\"; \"B\"; \"B\"; \"B\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"|],\n",
"    predictions = [|\"A\"; \"A\"; \"A\"; \"B\"; \"C\"; \"B\"; \"B\"; \"A\"; \"C\"; \"C\"; \"C\"; \"C\"; \"A\"; \"A\"|]\n",
")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["### Creating threshold-dependent metric maps\n",
"\n",
"Predictions usually have a confidence or score attached, which indicates how \"sure\" the predictor is to report a label for a certain input.\n",
"\n",
"Predictors can be compared by comparing the relative frequency distributions of metrics of interest for each possible (or obtained) confidence value.\n",
"\n",
"Two prominent examples are the **Receiver Operating Characteristic (ROC)** or the **Precision-Recall metric**\n",
"\n",
"#### For binary predictions\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 19, "outputs": [
          {
           "data": {
            "text/plain": ["Threshold 1.9:",
"\tSensitivity: 0.00",
"\tPrecision : NaN",
"\tFallout : 0.00",
"\tetc...",
"Threshold 0.9:",
"\tSensitivity: 0.25",
"\tPrecision : 1.00",
"\tFallout : 0.00",
"\tetc...",
"Threshold 0.7:",
"\tSensitivity: 0.50",
"\tPrecision : 0.67",
"\tFallout : 0.33",
"\tetc...",
"Threshold 0.6:",
"\tSensitivity: 0.75",
"\tPrecision : 0.75",
"\tFallout : 0.33",
"\tetc...",
"Threshold 0.3:",
"\tSensitivity: 0.75",
"\tPrecision : 0.60",
"\tFallout : 0.67",
"\tetc...",
"Threshold 0.2:",
"\tSensitivity: 1.00",
"\tPrecision : 0.67",
"\tFallout : 0.67",
"\tetc...",
"Threshold 0.1:",
"\tSensitivity: 1.00",
"\tPrecision : 0.57",
"\tFallout : 1.00",
"\tetc..."]
        },
           "execution_count": 19,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["ComparisonMetrics.binaryThresholdMap(\n",
"    [true;true;true;true;false;false;false],\n",
"    [0.9 ;0.6 ;0.7 ; 0.2 ; 0.7; 0.3 ; 0.1]\n",
")\n",
"|\u003e Array.iter (fun (threshold,cm) -\u003e printf $\"Threshold {threshold}:\\n\\tSensitivity: %.2f{cm.Sensitivity}\\n\\tPrecision : %.2f{cm.Precision}\\n\\tFallout : %.2f{cm.FallOut}\\n\\tetc...\\n\")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### For multi-label predictions\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 20, "outputs": [],
           "source": ["ComparisonMetrics.multiLabelThresholdMap(\n",
"    actual = \n",
"             [|\"A\"; \"A\"; \"A\"; \"A\"; \"A\"; \"B\"; \"B\"; \"B\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"|],\n",
"    predictions = [|\n",
"        \"A\", [|0.8; 0.7; 0.9; 0.4; 0.3; 0.1; 0.3; 0.5; 0.1; 0.1; 0.1; 0.3; 0.5; 0.4|]\n",
"        \"B\", [|0.0; 0.2; 0.0; 0.5; 0.1; 0.8; 0.7; 0.4; 0.0; 0.1; 0.1; 0.0; 0.1; 0.3|]\n",
"        \"C\", [|0.2; 0.3; 0.1; 0.1; 0.6; 0.1; 0.1; 0.1; 0.9; 0.8; 0.8; 0.7; 0.4; 0.3|]\n",
"    |]\n",
")\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### ROC curve example\n",
"\n",
"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
"\n",
"The ROC curve is created by plotting the true positive rate (TPR, sensitivity) against the false positive rate (FPR, fallout) at various threshold settings\n",
"\n",
"When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming \u0027positive\u0027 ranks higher than \u0027negative\u0027).\n",
"In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which.\n",
"\n",
"##### Binary\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 21, "outputs": [],
           "source": ["open Plotly.NET\n",
"open Plotly.NET.LayoutObjects\n",
"open FSharp.Stats.Integration\n",
"\n",
"let binaryROC = \n",
"    ComparisonMetrics.calculateROC(\n",
"        [true;true;true;true;false;false;false],\n",
"        [0.9 ;0.6 ;0.7 ; 0.2 ; 0.7; 0.3 ; 0.1]\n",
"    )\n",
"\n",
"let auc = binaryROC |\u003e NumericalIntegration.definiteIntegral Trapezoidal\n",
"\n",
"let binaryROCChart =\n",
"    [\n",
"        Chart.Line(binaryROC, Name= $\"2 label ROC, AUC = %.2f{auc}\")\n",
"        |\u003e Chart.withLineStyle(Shape = StyleParam.Shape.Vh)\n",
"        Chart.Line([0.,0.; 1.,1.0], Name = \"no skill\", LineDash = StyleParam.DrawingStyle.Dash, LineColor = Color.fromKeyword Grey)\n",
"    ]\n",
"    |\u003e Chart.combine\n",
"    |\u003e Chart.withTemplate ChartTemplates.lightMirrored\n",
"    |\u003e Chart.withLegend(Legend.init(XAnchor=StyleParam.XAnchorPosition.Right, YAnchor=StyleParam.YAnchorPosition.Bottom, X = 0.5, Y = 0.1))\n",
"    |\u003e Chart.withXAxisStyle(\"TPR\", MinMax=(0.,1.))\n",
"    |\u003e Chart.withYAxisStyle(\"FPR\", MinMax=(0.,1.))\n",
"    |\u003e Chart.withTitle \"Binary receiver operating characteristic example\"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": null, "outputs": [
          {
           "data": {
            "text/html": ["\u003cdiv id=\"6468a2be-af88-4dee-a7c5-3afa4c14f4c7\"\u003e\u003c!-- Plotly chart will be drawn inside this DIV --\u003e\u003c/div\u003e",
"",
"\u003cscript type=\"text/javascript\"\u003e",
"",
"",
"",
"            var renderPlotly_6468a2beaf884deea7c53afa4c14f4c7 = function() {",
"",
"            var fsharpPlotlyRequire = requirejs.config({context:\u0027fsharp-plotly\u0027,paths:{plotly:\u0027https://cdn.plot.ly/plotly-2.6.3.min\u0027}}) || require;",
"",
"            fsharpPlotlyRequire([\u0027plotly\u0027], function(Plotly) {",
"",
"",
"",
"            var data = [{\"type\":\"scatter\",\"name\":\"2 label ROC, AUC = 0.71\",\"mode\":\"lines\",\"x\":[0.0,0.0,0.3333333333333333,0.3333333333333333,0.6666666666666666,0.6666666666666666,1.0],\"y\":[0.0,0.25,0.5,0.75,0.75,1.0,1.0],\"marker\":{},\"line\":{\"shape\":\"vh\"}},{\"type\":\"scatter\",\"name\":\"no skill\",\"mode\":\"lines\",\"x\":[0.0,1.0],\"y\":[0.0,1.0],\"marker\":{},\"line\":{\"color\":\"rgba(128, 128, 128, 1.0)\",\"dash\":\"dash\"}}];",
"",
"            var layout = {\"width\":600,\"height\":600,\"template\":{\"layout\":{\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"xaxis\":{\"ticks\":\"inside\",\"mirror\":\"all\",\"showline\":true,\"zeroline\":true},\"yaxis\":{\"ticks\":\"inside\",\"mirror\":\"all\",\"showline\":true,\"zeroline\":true}},\"data\":{}},\"legend\":{\"x\":0.5,\"y\":0.1,\"yanchor\":\"bottom\"},\"xaxis\":{\"title\":{\"text\":\"TPR\"},\"range\":[0.0,1.0]},\"yaxis\":{\"title\":{\"text\":\"FPR\"},\"range\":[0.0,1.0]},\"title\":{\"text\":\"Binary receiver operating characteristic example\"}};",
"",
"            var config = {\"responsive\":true};",
"",
"            Plotly.newPlot(\u00276468a2be-af88-4dee-a7c5-3afa4c14f4c7\u0027, data, layout, config);",
"",
"});",
"",
"            };",
"",
"            if ((typeof(requirejs) !==  typeof(Function)) || (typeof(requirejs.config) !== typeof(Function))) {",
"",
"                var script = document.createElement(\"script\");",
"",
"                script.setAttribute(\"src\", \"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\");",
"",
"                script.onload = function(){",
"",
"                    renderPlotly_6468a2beaf884deea7c53afa4c14f4c7();",
"",
"                };",
"",
"                document.getElementsByTagName(\"head\")[0].appendChild(script);",
"",
"            }",
"",
"            else {",
"",
"                renderPlotly_6468a2beaf884deea7c53afa4c14f4c7();",
"",
"            }",
"",
"\u003c/script\u003e",
"",
""]
        },
           "execution_count": null,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["binaryROCChart\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["##### Multi-label\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 23, "outputs": [],
           "source": ["let multiLabelROC = \n",
"    ComparisonMetrics.calculateMultiLabelROC(\n",
"        actual = [|\"A\"; \"A\"; \"A\"; \"A\"; \"A\"; \"B\"; \"B\"; \"B\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"; \"C\"|],\n",
"        predictions = [|\n",
"            \"A\", [|0.8; 0.7; 0.9; 0.4; 0.3; 0.1; 0.2; 0.5; 0.1; 0.1; 0.1; 0.3; 0.5; 0.4|]\n",
"            \"B\", [|0.0; 0.1; 0.0; 0.5; 0.1; 0.8; 0.7; 0.4; 0.0; 0.1; 0.1; 0.0; 0.1; 0.3|]\n",
"            \"C\", [|0.2; 0.2; 0.1; 0.1; 0.6; 0.1; 0.1; 0.1; 0.9; 0.8; 0.8; 0.7; 0.4; 0.3|]\n",
"        |]\n",
"    )\n",
"\n",
"let aucMap = \n",
"    multiLabelROC \n",
"    |\u003e Map.map (fun label roc -\u003e roc |\u003e NumericalIntegration.definiteIntegral Trapezoidal)\n",
"\n",
"let multiLabelROCChart =\n",
"    [\n",
"        yield!  \n",
"            multiLabelROC\n",
"            |\u003e Map.toArray\n",
"            |\u003e Array.map (fun (label,roc) -\u003e \n",
"                Chart.Line(roc, Name= $\"{label} ROC, AUC = %.2f{aucMap[label]}\")\n",
"                |\u003e Chart.withLineStyle(Shape = StyleParam.Shape.Vh)\n",
"            )\n",
"        Chart.Line([0.,0.; 1.,1.0], Name = \"no skill\", LineDash = StyleParam.DrawingStyle.Dash, LineColor = Color.fromKeyword Grey)\n",
"    ]\n",
"    |\u003e Chart.combine\n",
"    |\u003e Chart.withTemplate ChartTemplates.lightMirrored\n",
"    |\u003e Chart.withLegend(Legend.init(XAnchor=StyleParam.XAnchorPosition.Right, YAnchor=StyleParam.YAnchorPosition.Bottom, X = 0.5, Y = 0.1))\n",
"    |\u003e Chart.withXAxisStyle(\"TPR\", MinMax=(0.,1.))\n",
"    |\u003e Chart.withYAxisStyle(\"FPR\", MinMax=(0.,1.))\n",
"    |\u003e Chart.withTitle \"Binary receiver operating characteristic example\"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": null, "outputs": [
          {
           "data": {
            "text/html": ["\u003cdiv id=\"3b31b907-ab0a-489e-aa6c-ecb9297d17d3\"\u003e\u003c!-- Plotly chart will be drawn inside this DIV --\u003e\u003c/div\u003e",
"",
"\u003cscript type=\"text/javascript\"\u003e",
"",
"",
"",
"            var renderPlotly_3b31b907ab0a489eaa6cecb9297d17d3 = function() {",
"",
"            var fsharpPlotlyRequire = requirejs.config({context:\u0027fsharp-plotly\u0027,paths:{plotly:\u0027https://cdn.plot.ly/plotly-2.6.3.min\u0027}}) || require;",
"",
"            fsharpPlotlyRequire([\u0027plotly\u0027], function(Plotly) {",
"",
"",
"",
"            var data = [{\"type\":\"scatter\",\"name\":\"A ROC, AUC = 0.87\",\"mode\":\"lines\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.2222222222222222,0.3333333333333333,0.4444444444444444,0.5555555555555556,1.0,1.0],\"y\":[0.0,0.2,0.4,0.6,0.6,0.6,0.8,1.0,1.0,1.0,1.0],\"marker\":{},\"line\":{\"shape\":\"vh\"}},{\"type\":\"scatter\",\"name\":\"B ROC, AUC = 0.97\",\"mode\":\"lines\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.09090909090909091,0.09090909090909091,0.18181818181818182,0.18181818181818182,0.6363636363636364,1.0],\"y\":[0.0,0.0,0.3333333333333333,0.6666666666666666,0.6666666666666666,0.6666666666666666,1.0,1.0,1.0,1.0,1.0],\"marker\":{},\"line\":{\"shape\":\"vh\"}},{\"type\":\"scatter\",\"name\":\"C ROC, AUC = 0.96\",\"mode\":\"lines\",\"x\":[0.0,0.0,0.0,0.0,0.125,0.125,0.125,0.125,0.375,1.0,1.0],\"y\":[0.0,0.16666666666666666,0.5,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.8333333333333334,1.0,1.0,1.0,1.0],\"marker\":{},\"line\":{\"shape\":\"vh\"}},{\"type\":\"scatter\",\"name\":\"macro-average ROC, AUC = 0.94\",\"mode\":\"lines\",\"x\":[0.0,0.0,0.0,0.0,0.041666666666666664,0.14604377104377106,0.18308080808080807,0.25042087542087543,0.37079124579124584,0.8787878787878788,1.0],\"y\":[0.0,0.12222222222222223,0.41111111111111115,0.6444444444444444,0.6444444444444444,0.6444444444444444,0.8777777777777778,1.0,1.0,1.0,1.0],\"marker\":{},\"line\":{\"shape\":\"vh\"}},{\"type\":\"scatter\",\"name\":\"micro-average ROC, AUC = 0.93\",\"mode\":\"lines\",\"x\":[0.0,0.0,0.0,0.0,0.03571428571428571,0.14285714285714285,0.17857142857142858,0.25,0.35714285714285715,0.8571428571428571,1.0],\"y\":[0.0,0.14285714285714285,0.42857142857142855,0.6428571428571429,0.6428571428571429,0.6428571428571429,0.8571428571428571,1.0,1.0,1.0,1.0],\"marker\":{},\"line\":{\"shape\":\"vh\"}},{\"type\":\"scatter\",\"name\":\"no skill\",\"mode\":\"lines\",\"x\":[0.0,1.0],\"y\":[0.0,1.0],\"marker\":{},\"line\":{\"color\":\"rgba(128, 128, 128, 1.0)\",\"dash\":\"dash\"}}];",
"",
"            var layout = {\"width\":600,\"height\":600,\"template\":{\"layout\":{\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"xaxis\":{\"ticks\":\"inside\",\"mirror\":\"all\",\"showline\":true,\"zeroline\":true},\"yaxis\":{\"ticks\":\"inside\",\"mirror\":\"all\",\"showline\":true,\"zeroline\":true}},\"data\":{}},\"legend\":{\"x\":0.5,\"y\":0.1,\"yanchor\":\"bottom\"},\"xaxis\":{\"title\":{\"text\":\"TPR\"},\"range\":[0.0,1.0]},\"yaxis\":{\"title\":{\"text\":\"FPR\"},\"range\":[0.0,1.0]},\"title\":{\"text\":\"Binary receiver operating characteristic example\"}};",
"",
"            var config = {\"responsive\":true};",
"",
"            Plotly.newPlot(\u00273b31b907-ab0a-489e-aa6c-ecb9297d17d3\u0027, data, layout, config);",
"",
"});",
"",
"            };",
"",
"            if ((typeof(requirejs) !==  typeof(Function)) || (typeof(requirejs.config) !== typeof(Function))) {",
"",
"                var script = document.createElement(\"script\");",
"",
"                script.setAttribute(\"src\", \"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\");",
"",
"                script.onload = function(){",
"",
"                    renderPlotly_3b31b907ab0a489eaa6cecb9297d17d3();",
"",
"                };",
"",
"                document.getElementsByTagName(\"head\")[0].appendChild(script);",
"",
"            }",
"",
"            else {",
"",
"                renderPlotly_3b31b907ab0a489eaa6cecb9297d17d3();",
"",
"            }",
"",
"\u003c/script\u003e",
"",
""]
        },
           "execution_count": null,
           "metadata": {},
           "output_type": "execute_result"
          }],
           "source": ["multiLabelROCChart\n"]
          }],
            "metadata": {
            "kernelspec": {"display_name": ".NET (F#)", "language": "F#", "name": ".net-fsharp"},
            "langauge_info": {
        "file_extension": ".fs",
        "mimetype": "text/x-fsharp",
        "name": "C#",
        "pygments_lexer": "fsharp",
        "version": "4.5"
        }
        },
            "nbformat": 4,
            "nbformat_minor": 1
        }
        

